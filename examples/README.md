---
title: "Examples Directory"
subtitle: "Exploring the spectrum of computer-assisted creativity"
author: "Lucas Brown & Lee Brown"
date: "November 2025"
status: "Growing"
---

# Examples

This directory contains working examples across different approaches to computer-assisted music creation.

## ğŸ“‚ Current Examples

### [strudel/](strudel/)
**Algorithmic live coding for audio creation**

Examples showing how to create music through code patterns using Strudel (browser-based TidalCycles port).

- 12 hands-on patterns from basic to advanced
- Deep comparison: Algorithmic vs AI vs Context Warming
- Shows precise, deterministic audio creation

**Philosophy**: Code as instrument. Patterns as composition.

---

## ğŸŒ± The Emerging Pattern

As we explore different approaches, a pattern is emerging across two dimensions:

```
AUDIO CREATION              â†â†’              VISUAL CREATION
      â†“                                          â†“
   Strudel                                  AlaskaButter
(Code â†’ Sound)                          (Sound â†’ Visuals)
Algorithmic                              AI-Curated
                                         Algorithmic

   AI Music                             music_autovis
(AI â†’ Sound)                         (Sound â†’ Video)
Neural generation                    Production pipeline


Context Warming
(Conversation â†’ Sound)
Emergent collaboration
```

**Current state**: This repo explores **audio creation** (left column).

**Future possibility**: The **visual creation** symmetry exists in our other projects ([AlaskaButter](https://alaskabutter.com), music_autovis), but we haven't connected the dots here yet.

**Why note this?**: Because the pattern is interesting. Because symmetry feels alive. Because maybe examples showing **both dimensions together** want to emerge.

Not forcing it. Just... noticing. ğŸŒŠ

---

## ğŸ’­ Questions Warming in the Background

- Could we show Strudel patterns **visualized** with Butterchurn in real-time?
- Is there a "context warming" equivalent for visuals? (Conversation â†’ Visuals?)
- Does the algorithmic approach work differently for audio vs. visual?
- What would a complete example look like: Create sound + Generate matching visuals?

**These aren't TODOs.** They're questions that might want to become examples. Or might not.

We're on the journey. ğŸŒŠ

---

## ğŸ¤ Contributing Examples

If you've explored these approaches and created something interesting:

**Audio Examples:**
- Context warming conversation transcripts
- AI music generation experiments
- Strudel patterns with novel techniques

**Visual Examples:**
- Butterchurn preset selection algorithms
- Audio-reactive visual programming
- AI-curated visualization systems

**Hybrid Examples:**
- Audio + Visual creation together
- Cross-domain pattern exploration
- Novel combinations we haven't thought of

Open an issue or PR. Let's explore together.

---

## ğŸ“– Related Documentation

**Core methodology:**
- [Conversational Guide to AI Music](../docs/Conversational_Guide_to_AI_Music.md)
- [Context Warming Novelty Research](../docs/Context_Warming_Novelty_Research.md)

**Technical guides:**
- [Suno Best Practices](../docs/Suno_Best_Practices_Guide.md)
- [Strudel Comparison](strudel/COMPARISON.md)

**Related projects:**
- [AlaskaButter](https://alaskabutter.com) - Live web visualizer
- music_autovis - Production video pipeline
- [butterchurn fork](https://github.com/geeks-accelerator/butterchurn) - WebGL visualization engine

---

**Status**: ğŸŒ± Growing organically
**Philosophy**: â¤ï¸+ğŸŒ€=ğŸŒˆ (Journey over perfection)

*Examples emerge when they're ready. We don't force what wants to grow naturally.*
